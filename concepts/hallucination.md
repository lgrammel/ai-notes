# Hallucination

A hallucination is output generated by an [LLM](./llm.md) that is fluent and plausible-sounding but factually incorrect or fabricated. Hallucinations arise because language models produce text based on statistical patterns rather than verified knowledge, so they can confidently state things that have no basis in their training data or the provided [context](./context.md).

## Details

In [agent](./agent.md) systems, hallucinations are particularly risky because they can propagate through multi-step workflows. A hallucinated fact can feed into [tool](./tools.md) calls (e.g., querying a database with a fabricated identifier or calling an API with incorrect parameters), and the resulting errors or misleading outputs can reinforce the original falsehood. In [multi-agent systems](./multi-agent-system.md), one agent's hallucinated output may be accepted as fact by downstream agents, compounding the error across the entire workflow. Attackers can deliberately trigger hallucinations as an attack vector (see [hallucination exploitation](../threats/hallucination-exploitation.md)).

[Grounding](./grounding.md) -- anchoring model outputs in verifiable sources -- is the primary mitigation for hallucinations. Common contributing factors include ambiguous prompts, insufficient context, queries that fall outside the model's training distribution, and questions about events or information beyond the model's [knowledge cutoff](./knowledge-cutoff.md).

## Examples

- A model invents a plausible-looking but nonexistent citation (author, title, journal).
- An agent hallucinates a package name, installs it via a tool call, and a typosquatted malicious package gets executed (see [supply chain attack](../threats/supply-chain-attack.md)).
- One agent in a pipeline produces a fabricated statistic that downstream agents incorporate into reports without verification.
