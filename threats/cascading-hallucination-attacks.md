# Cascading Hallucination Attacks

Cascading Hallucination Attacks exploit an AI's tendency to generate contextually plausible but false information, which can propagate through systems and disrupt decision-making. This can also lead to destructive reasoning affecting [tools](../concepts/tools.md) invocation.
